from transformers import BertTokenizer, TFBertModel
import numpy as np

import json
import os


def get_bert_embedding(input_text: str):
    """Returns the embedding of the input text generated by BERT.

    Args:
        input_text (str): The text for which to generate the embedding.
    """

    print("Generating BERT embedding for text: " + input_text)

    input_text = input_text.lower()
    input_text = remove_stopwords(input_text)

    tokenizer = BertTokenizer.from_pretrained("bert-base-uncased")
    model = TFBertModel.from_pretrained("bert-base-uncased")

    encoded_input = tokenizer(input_text, return_tensors="tf")
    output_embeddings = model(encoded_input).pooler_output.numpy()

    return output_embeddings


def remove_stopwords(input_text: str):
    """Removes stopwords from the input text.

    Args:
        input_text (str): The text from which to remove stopwords.
    """

    print("Removing stopwords from text: " + input_text)
    stopwords = ["a", "an", "the", "is", "are", "was", "were", "has", "have", "had"]
    for stopword in stopwords:
        input_text = input_text.replace(stopword, "")

    return input_text


def init_bucket_embeddings():
    buckets = [
        "Left",
        "Right",
        "Stationary",
        "Straight",
        "Straight-Left",
        "Straight-Right",
        "Right-U-Turn",
        "Left-U-Turn",
    ]
    # TODO Change magic paths to get paths from config file

    bucket_embeddings = {}

    # Check if embeddings have already been initialized
    if not os.path.exists("datasets/bucket_embeddings.json"):
        bucket_embeddings = {}
        for bucket in buckets:
            embedding = get_bert_embedding(bucket.lower())
            # Convert ndarray to a list
            bucket_embeddings[bucket] = embedding.tolist()[0]
            print(len(bucket_embeddings[bucket]))

        # Save to JSON file
        with open("datasets/bucket_embeddings.json", "w") as json_file:
            json.dump(bucket_embeddings, json_file, indent=4)
    else:
        with open("datasets/bucket_embeddings.json", "r") as file:
            bucket_embeddings = json.load(file)

    return bucket_embeddings


def test_bert_encoding(input_text: str) -> dict:
    buckets = [
        "Left",
        "Right",
        "Stationary",
        "Straight",
        "Straight-Left",
        "Straight-Right",
        "Right-U-Turn",
        "Left-U-Turn",
    ]

    input_text_embedding = get_bert_embedding(input_text)
    # Compute the dot product between query embedding and document embedding

    scores = []
    for bucket in buckets:
        bucket_embedding = get_bert_embedding(bucket.lower())[0]
        dot = np.dot(input_text_embedding, bucket_embedding)[0]
        norm = np.linalg.norm(input_text_embedding) * np.linalg.norm(bucket_embedding)
        print(dot)
        print(norm)
        scores.append(dot / norm)

    # Find the highest scores
    max_idx = np.argsort(-np.array(scores))

    # print(f"Query: {input_text}")
    # for idx in max_idx:
    #     print(f"Score: {scores[idx]:.2f}")
    #     print(buckets[idx])
    #     print("--------")

    # Create dictionary for return
    output_dict = {}
    for idx in max_idx:
        output_dict[buckets[idx]] = scores[idx]

    return output_dict
